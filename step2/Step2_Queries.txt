QUERY 1 (Courses):
{
  "search": "Selenium",
  "filter": "role eq 'developer'",
  "select": "title,description,duration,instructor,level,product,rating_average,rating_count,role,source,url,keyphrases"
}

RESULT 1:
{
  "@odata.context": "https://dentalofficeassistant-search.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 9.329301,
      "description": "Run Selenium UI tests, a form of functional testing, in Azure Pipelines.",
      "duration": 63,
      "instructor": null,
      "level": "beginner",
      "product": "azure-devops",
      "rating_average": 4.66,
      "rating_count": 800,
      "role": "developer",
      "source": "MS Learn",
      "title": "Run functional tests in Azure Pipelines",
      "url": "https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Selenium UI tests",
        "functional testing",
        "Azure Pipelines",
        "form"
      ]
    },
    {
      "@search.score": 9.329301,
      "description": "Run Selenium UI tests, a form of functional testing, in Azure Pipelines.",
      "duration": 63,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 800,
      "role": "developer",
      "source": "MS Learn",
      "title": "Run functional tests in Azure Pipelines",
      "url": "https://docs.microsoft.com/en-us/learn/modules/run-functional-tests-azure-pipelines/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Selenium UI tests",
        "functional testing",
        "Azure Pipelines",
        "form"
      ]
    }
  ]
}


QUERY 2 (Library):
{
  "search": "e-commerce",
  "select": "metadata_author,metadata_title,metadata_creation_date,content,organizations,people",
  "top": 3
}

RESULT 2:
{
  "@odata.context": "https://dentalofficeassistant-search.search.windows.net/indexes('papers-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 14.689963,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "organizations": [
        "Security Evaluation Center",
        "rapid development",
        "Internet society of China",
        "Taobao",
        "cial e-commerce",
        "EURASIP Journal",
        "EURASIP",
        "tional neural network",
        "CNN",
        "ELMO",
        "content curation social network",
        "CCSN",
        "Bayes",
        "industry",
        "ory",
        "social apps",
        "information collection service",
        "col-",
        "application layer",
        "social software",
        "ule",
        "Social",
        "IPC",
        "IPC Backgroud",
        "real system service",
        "auxiliary",
        "social",
        "background ser",
        "next",
        "TF-IDF",
        "Jieba",
        "IDF",
        "TFIDF",
        "commerce data",
        "BERT",
        "RNN",
        "MLM",
        "GPT",
        "Google",
        "verification",
        "TF",
        "tains"
      ],
      "people": [
        "Haoliang Cui1",
        "Shuai Shao2",
        "Shaozhang Niu1",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Cui",
        "Bengio",
        "Kim",
        "Liu",
        "Peters",
        "Devlin",
        "Wu",
        "Malli",
        "Chen",
        "Ning",
        "Yin",
        "nally",
        "Binder",
        "Bayes",
        "phrase",
        "j D j",
        "j"
      ]
    },
    {
      "@search.score": 3.5276198,
      "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like p",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2013-10-11T02:33:25Z",
      "organizations": [
        "Brazilian Computer Society",
        "ric technologies",
        "dynamics",
        "keystroke dynamics",
        "ficial intelligence",
        "state",
        "Instituto de Ciências Matemáticas e de Computação",
        "ICMC",
        "Universidade de São Paulo",
        "USP",
        "Instituto de Ciência e Tecnologia",
        "ICT",
        "Universidade Federal de São Paulo",
        "UNIFESP",
        "col",
        "systematic review",
        "IDS",
        "intrusion detection sys",
        "keystroke",
        "tocol",
        "ing",
        "J Braz Comput Soc",
        "ACM Digital Library",
        "Science Direct",
        "Scopus",
        "ics",
        "cate",
        "lic",
        "Comput Soc"
      ],
      "people": [
        "J Braz",
        "Paulo Henrique Pisani",
        "Ana Carolina Lorena",
        "P. H. Pisani",
        "A. C. Lorena",
        "ularly",
        "criteria",
        "Mendeley",
        "Gaines",
        "gally"
      ]
    },
    {
      "@search.score": 2.0082538,
      "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data  (2018) 5:33  \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence:   \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\n\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n  • K anonymity\n  • L diversity\n  • T closeness\n  • Randomization\n  • Data distribution\n  • Cryptographic techniques\n  • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n  • More number of Mappers and Reducers were used as data volume increased.\n  • Results before and after randomization were significantly different.\n  • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n  • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018   Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Pro",
      "metadata_author": "P. Ram Mohan Rao ",
      "metadata_title": "Privacy preservation techniques in big data analytics: a survey",
      "metadata_creation_date": "2018-09-20T05:58:23Z",
      "organizations": [
        "Amazon",
        "Flip",
        "social",
        "MLR Institute of Technology",
        "contribut",
        "health department",
        "Generalization",
        "Health Department",
        "analytics",
        "nisms",
        "preservation",
        "European Union",
        "CCTV",
        "APSK",
        "Science and Engineering",
        "Sri Venkateswara College of Engineering",
        "Springer Nature",
        "Soft Comput",
        "Knowl Inf Syst.",
        "lan",
        "IEEE",
        "ACM",
        "SRI International",
        "Int J",
        "Vision",
        "Springer",
        "BCM",
        "J Big Data",
        "IBM"
      ],
      "people": [
        "P. Ram Mohan Rao",
        "S. Murali Krishna2",
        "A. P. Siva Kumar3",
        "kart",
        "Ram Mohan Rao",
        "12Ram Mohan Rao",
        "John",
        "K",
        "12Ram",
        "Rao",
        "Job",
        "Pietro",
        "Pecori Riccardo",
        "Mezzina Paolo",
        "Chauhan Arun",
        "Kummamuru Krishna",
        "Toshniwal Durga",
        "Yang D",
        "Bingqing Q",
        "Cudre-Mauroux P.",
        "Liu Y",
        "Duncan GT",
        "Diane L.",
        "Lambert Diane",
        "Spiller K",
        "Hettig M",
        "Kiss E",
        "Kassel J-F",
        "Weber S",
        "Harbach M",
        "Smith M",
        "Bayardo RJ",
        "Agrawal A.",
        "ence",
        "Iyengar S.",
        "LeFevre K",
        "DeWitt",
        "Ramakrishnan R",
        "DeWitt DJ",
        "Ramakrishnan R. Mondrian",
        "Samarati",
        "Pierangela",
        "Latanya Sweeney",
        "Sweeney Latanya",
        "J Uncertain",
        "Williams R.",
        "Machanavajjhala A",
        "Xiao X",
        "Yufei T.",
        "Rubner Y",
        "Tomasi T",
        "Guibas LJ",
        "Aggarwal CC",
        "Philip SY",
        "Jiang R",
        "Lu R",
        "Choo KK",
        "Wang K",
        "Yu PS",
        "Chakraborty S.",
        "Fung",
        "Zhang X",
        "Al-Zobbi M",
        "Shahrestani S",
        "Ruan C.",
        "Ruan C",
        "Schneider C."
      ]
    }
  ]
}